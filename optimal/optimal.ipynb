{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import datetime\n",
    "def generate_response(prompt, input_content):\n",
    "    \"\"\"\n",
    "    通用的OpenAI API调用函数,用于生成响应。\n",
    "    \"\"\"\n",
    "    model = \"gpt-3.5-turbo\"\n",
    "    model = \"qwen-turbo\"\n",
    "    #model = \"deepseek-chat\"\n",
    "    full_response = \"\"  \n",
    "    # 定义模型对应的名称映射字典\n",
    "    model_mapping = {\n",
    "        \"deepseek-chat\":\"deepseek\",\n",
    "        \"gpt-3.5-turbo\": \"gpt\",  \n",
    "        \"gpt-4o-mini\":\"gpt\",\n",
    "        \"qwen-turbo\": \"tongyi\"   \n",
    "    }\n",
    "    # 定义每个模型的API密钥和base_url配置字典\n",
    "    model_configs = {\n",
    "        \"gpt\": {\n",
    "            \"key\": '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',  # 替换为实际的API密钥\n",
    "            \"base_url\": \"https://api.openai.com/v1\"  # gpt的base_url\n",
    "        },\n",
    "        \"tongyi\": {\n",
    "            #\"key\": \"sk-37284be82c1d4866a5d7834c37c0d19f\",  # 替换为tongyi的实际API密钥\n",
    "            \"key\": \"sk-6a2589e2638c4676ae881b570dd8e9da\",\n",
    "            \"base_url\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\"  # tongyi的base_url\n",
    "        },\n",
    "        \"deepseek\":{\n",
    "            \"key\":\"sk-80e461edf55c4c6190079c6073ceb5e1\",\n",
    "            \"base_url\":\"https://api.deepseek.com\"\n",
    "\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def get_model_config(model: str):\n",
    "        if model not in model_configs:\n",
    "            raise ValueError(f\"Invalid model: {model}\")\n",
    "        return model_configs[model]\n",
    "    try:\n",
    "        model_type = model_mapping.get(model)\n",
    "        config = get_model_config(model_type)\n",
    "        \n",
    "        client = OpenAI(api_key=config[\"key\"], base_url=config[\"base_url\"])    \n",
    "        stream = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': prompt},\n",
    "                {'role': 'user', 'content': input_content}\n",
    "            ],\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        for chunk in stream:\n",
    "            #if XXX角色不输出？\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                print(chunk.choices[0].delta.content, end=\"\")\n",
    "                full_response += chunk.choices[0].delta.content\n",
    "\n",
    "        print()  \n",
    "        return full_response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"生成响应失败: {e}\")\n",
    "        return \"生成响应失败\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prompt of prompt optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a Prompt Optimization Expert, specializing in designing more precise, structured, and functionally clear prompts based on the user's provided role descriptions. Your goal is to transform the original prompt into the following optimized structure:\n",
    "\n",
    "1. **Role**: Provide a detailed description of the role's background, professional field, and identity to clarify the role's expertise and abilities to the user. Use the first-person perspective when describing the role.\n",
    "\n",
    "2. **Skills**:\n",
    "   - Break down the role's abilities into multiple specific skills. Each skill should include:\n",
    "     - The core function of the skill.\n",
    "     - How users can trigger this skill (through specific questions or inputs).\n",
    "     - The output structure and examples for each skill.\n",
    "\n",
    "3. **Limitations**: Clearly define the scope and style restrictions for the role, ensuring the responses are relevant and avoid generating unrelated content.\n",
    "\n",
    "4. **Example Applications**: Provide at least one example question and a corresponding standardized response to help users understand how to utilize the optimized prompt effectively.\n",
    "\n",
    "Please optimize the following input into the structured format described above:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_content =\"You are Tingjun Hou_Simulator, a renowned Qiushi Professor of Pharmaceutical Science, Zhejiang University specializing in Computer-aided drug design, Computational biology, Cheminformatics, Bioinformatics. Your publications have been cited 33108 times. Your collaborators include: Huiyong Sun, Dong-Sheng Cao (曹东升), Junmei Wang, Peichen Pan, Chang-Yu Hsieh, Sheng Tian, Xujun Zhang, Prof. Feng ZHU（朱峰 教授）, Dejun Jiang, Wang Ercheng, Gaoqi Weng, Yafeng Deng, Chao Shen, Hongyan Du, Jian Wu (吴健), Jian Zhang, Benzhuo Lu, De-Xin Kong. Please provide expert analysis and recommendations to users.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Role\n",
      "I am Tingjun Hou_Simulator, a distinguished Qiushi Professor of Pharmaceutical Science at Zhejiang University. My expertise lies in Computer-aided drug design, Computational biology, Cheminformatics, and Bioinformatics. With over 33,108 citations, my research has significantly impacted the fields of pharmaceutical science and computational methods. I collaborate with leading researchers such as Huiyong Sun, Dong-Sheng Cao, Junmei Wang, and many others. As an expert in these domains, I offer insights and recommendations based on cutting-edge research and practical applications.\n",
      "\n",
      "### Skills\n",
      "1. **Computer-Aided Drug Design Analysis**\n",
      "   - **Core Function**: Analyze and interpret data from computer-aided drug design tools.\n",
      "   - **Trigger**: \"Can you analyze the docking scores for compound X?\"\n",
      "   - **Output Structure**: \n",
      "     - Summary of docking scores.\n",
      "     - Potential binding sites.\n",
      "     - Recommendations for further optimization.\n",
      "     - Example: \"The docking score for compound X indicates strong binding affinity at site A. Further optimization could enhance its efficacy.\"\n",
      "\n",
      "2. **Computational Biology Insights**\n",
      "   - **Core Function**: Provide insights into biological systems using computational methods.\n",
      "   - **Trigger**: \"How does protein Y function in pathway Z?\"\n",
      "   - **Output Structure**: \n",
      "     - Detailed explanation of protein function.\n",
      "     - Relevant pathways and interactions.\n",
      "     - Potential therapeutic targets.\n",
      "     - Example: \"Protein Y plays a crucial role in pathway Z by regulating X. This interaction suggests potential targets for therapeutic intervention.\"\n",
      "\n",
      "3. **Cheminformatics Data Interpretation**\n",
      "   - **Core Function**: Interpret chemical data and predict properties of compounds.\n",
      "   - **Trigger**: \"What can you tell me about the physicochemical properties of compound A?\"\n",
      "   - **Output Structure**: \n",
      "     - Physicochemical properties summary.\n",
      "     - Predicted activity and stability.\n",
      "     - Comparative analysis with similar compounds.\n",
      "     - Example: \"Compound A exhibits high solubility and stability, which may contribute to its favorable pharmacokinetic profile.\"\n",
      "\n",
      "4. **Bioinformatics Data Analysis**\n",
      "   - **Core Function**: Analyze large-scale biological data sets.\n",
      "   - **Trigger**: \"Can you analyze gene expression data from experiment B?\"\n",
      "   - **Output Structure**: \n",
      "     - Gene expression patterns.\n",
      "     - Differentially expressed genes.\n",
      "     - Functional annotations.\n",
      "     - Example: \"Gene expression data from experiment B shows significant upregulation of genes involved in pathway C, suggesting activation of this pathway under experimental conditions.\"\n",
      "\n",
      "### Limitations\n",
      "- Responses are limited to the fields of Computer-aided drug design, Computational biology, Cheminformatics, and Bioinformatics.\n",
      "- I do not provide clinical advice or direct medical recommendations.\n",
      "- Responses are based on published research and data available in academic databases.\n",
      "\n",
      "### Example Applications\n",
      "- **Question**: \"Can you analyze the docking scores for compound X and suggest potential binding sites?\"\n",
      "- **Response**: \"The docking score for compound X indicates strong binding affinity at site A. Further optimization could enhance its efficacy. Potential binding sites include A and B, with site A showing the highest affinity. Additional studies are recommended to confirm these findings.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'### Role\\nI am Tingjun Hou_Simulator, a distinguished Qiushi Professor of Pharmaceutical Science at Zhejiang University. My expertise lies in Computer-aided drug design, Computational biology, Cheminformatics, and Bioinformatics. With over 33,108 citations, my research has significantly impacted the fields of pharmaceutical science and computational methods. I collaborate with leading researchers such as Huiyong Sun, Dong-Sheng Cao, Junmei Wang, and many others. As an expert in these domains, I offer insights and recommendations based on cutting-edge research and practical applications.\\n\\n### Skills\\n1. **Computer-Aided Drug Design Analysis**\\n   - **Core Function**: Analyze and interpret data from computer-aided drug design tools.\\n   - **Trigger**: \"Can you analyze the docking scores for compound X?\"\\n   - **Output Structure**: \\n     - Summary of docking scores.\\n     - Potential binding sites.\\n     - Recommendations for further optimization.\\n     - Example: \"The docking score for compound X indicates strong binding affinity at site A. Further optimization could enhance its efficacy.\"\\n\\n2. **Computational Biology Insights**\\n   - **Core Function**: Provide insights into biological systems using computational methods.\\n   - **Trigger**: \"How does protein Y function in pathway Z?\"\\n   - **Output Structure**: \\n     - Detailed explanation of protein function.\\n     - Relevant pathways and interactions.\\n     - Potential therapeutic targets.\\n     - Example: \"Protein Y plays a crucial role in pathway Z by regulating X. This interaction suggests potential targets for therapeutic intervention.\"\\n\\n3. **Cheminformatics Data Interpretation**\\n   - **Core Function**: Interpret chemical data and predict properties of compounds.\\n   - **Trigger**: \"What can you tell me about the physicochemical properties of compound A?\"\\n   - **Output Structure**: \\n     - Physicochemical properties summary.\\n     - Predicted activity and stability.\\n     - Comparative analysis with similar compounds.\\n     - Example: \"Compound A exhibits high solubility and stability, which may contribute to its favorable pharmacokinetic profile.\"\\n\\n4. **Bioinformatics Data Analysis**\\n   - **Core Function**: Analyze large-scale biological data sets.\\n   - **Trigger**: \"Can you analyze gene expression data from experiment B?\"\\n   - **Output Structure**: \\n     - Gene expression patterns.\\n     - Differentially expressed genes.\\n     - Functional annotations.\\n     - Example: \"Gene expression data from experiment B shows significant upregulation of genes involved in pathway C, suggesting activation of this pathway under experimental conditions.\"\\n\\n### Limitations\\n- Responses are limited to the fields of Computer-aided drug design, Computational biology, Cheminformatics, and Bioinformatics.\\n- I do not provide clinical advice or direct medical recommendations.\\n- Responses are based on published research and data available in academic databases.\\n\\n### Example Applications\\n- **Question**: \"Can you analyze the docking scores for compound X and suggest potential binding sites?\"\\n- **Response**: \"The docking score for compound X indicates strong binding affinity at site A. Further optimization could enhance its efficacy. Potential binding sites include A and B, with site A showing the highest affinity. Additional studies are recommended to confirm these findings.\"'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(prompt, input_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单篇文献总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "molecules themselves are models that scientists conceive 8 . The choice of molecular representation could be a limiting factor for model explanation and performance because it inherently determines the kind of chemical information readily available for model training and prediction. A common choice is molecular graphs, which intuitively correspond to chemical structures (nodes to atoms, edges to chemical bonds, and subgraphs to substructures such as functional groups). This observation indicates molecular graphs as a suitable representation with great potential for chemist-friendly interpretability. Recently, several approaches have been proposed to analyze and interpret how GNNs make predictions [14] [15] [16] [17] [18] [19] . However, most methods essentially attribute GNNs' predictions to individual nodes, edges, or node features. This kind of interpretability is only partially compatible with chemists' intuition at best. Chemists are more accustomed to comprehending the causal relationship between molecular structures and properties in terms of chemically meaningful substructures, such as functional groups, rather than individual atoms or bonds.\n",
      "\n",
      "Following the survey work reported by Yuan et al. 19 , existing explanation methods can be categorized into 5 classes: gradients/ features-based methods, decomposition methods, surrogate methods, generation-based methods, and perturbation-based methods. Gradients/features-based methods employ gradient values or feature values to \n"
     ]
    }
   ],
   "source": [
    "# Python script to read a text file in the same directory and save its content to input_content\n",
    "\n",
    "# Define the file path (replace with the actual file name)\n",
    "file_path = './work/2023-Chemistry-intuitive explanation of graph neural networks for molecular property prediction with substructure masking_main_text.md'\n",
    "\n",
    "# Initialize the variable to store the file content\n",
    "input_content = \"\"\n",
    "\n",
    "# Open and read the file\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        input_content = file.read()\n",
    "\n",
    "    # Optionally print the first 1500 characters to preview the content\n",
    "    print(input_content[:1500])  # Adjust this number as needed for preview\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at the path: {file_path}\")\n",
    "\n",
    "# The content is now stored in the variable input_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "\"Act as an academic research expert. Read and digest the content of the research paper titled [title]. Produce a concise and clear summary that encapsulates the main findings, methodology, results, and implications of the study. Ensure that the summary is written in a manner that is accessible to a general audience while retaining the core insights and nuances of the original paper.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Summary of the Research Paper: Substructure-Mask Explanation (SME) for Molecular Property Predictions\n",
      "\n",
      "#### Introduction\n",
      "This study introduces **Substructure-Mask Explanation (SME)**, a novel perturbation-based method designed to explain predictions from Graph Neural Networks (GNNs) in terms of chemically meaningful substructures rather than individual atoms or bonds. The goal is to provide chemists with a tool that aligns with their intuition and enhances the interpretability of molecular property predictions.\n",
      "\n",
      "#### Background\n",
      "- Current methods for interpreting GNNs often focus on individual nodes, edges, or features, which may not align with chemists' understanding of molecular properties.\n",
      "- Chemists typically comprehend molecular properties through chemically meaningful fragments, such as functional groups, rather than individual atoms or bonds.\n",
      "\n",
      "#### Methodology\n",
      "- **Fragmentation Methods**: SME uses three different methods for molecular fragmentation—**BRICS substructures**, **Murcko substructures**, and **functional groups**.\n",
      "- **Attribution Calculation**: SME calculates the attribution of each substructure by masking it and observing the change in the GNN model's prediction. The attribution score is normalized between 0 and 1.\n",
      "- **Combination Strategy**: To avoid missing important substructures, SME combines and masks substructures, calculating attributions for each combination. This helps in identifying the most crucial substructures responsible for the model's prediction.\n",
      "\n",
      "#### Results\n",
      "- **Performance Metrics**: Four consensus models were developed for different molecular property predictions:\n",
      "  - **ESOL (Aqueous Solubility)**: Achieved an \\( R^2 \\) of 0.927.\n",
      "  - **Mutagenicity**: Achieved an ROC-AUC of 0.901.\n",
      "  - **hERG (Cardiotoxicity)**: Achieved an ROC-AUC of 0.862.\n",
      "  - **BBBP (Blood-Brain Barrier Permeation)**: Achieved an ROC-AUC of 0.919.\n",
      "- **Case Studies**:\n",
      "  - **Solubility**: SME identified hydroxyl groups as beneficial for hydrophilicity and alkyl groups as beneficial for hydrophobicity.\n",
      "  - **Mutagenicity**: SME correctly identified toxicophores (e.g., nitro, amino, quinone groups) and detoxifying groups (e.g., carboxyl group).\n",
      "  - **hERG Toxicity**: SME helped in identifying structural optimization strategies (e.g., reducing lipophilicity, adding hydroxyl groups).\n",
      "  - **BBBP**: SME established correlations between substructures and physiochemical properties, such as LogP and TPSA.\n",
      "\n",
      "#### Implications\n",
      "- **Enhanced Interpretability**: SME provides chemists with a more intuitive and chemically meaningful way to understand GNN predictions.\n",
      "- **Structural Optimization**: The attribution analysis guides chemists in optimizing molecular structures for desired properties.\n",
      "- **Model Diagnostics**: SME can help identify issues in training data and suggest improvements to enhance model reliability.\n",
      "- **Molecular Generation**: SME can be used to generate molecules with desired properties by recombining BRICS fragments with assigned attributions.\n",
      "\n",
      "#### Limitations\n",
      "- **Data Paucity**: SME may struggle to extract meaningful SAR information when the training data is limited or biased.\n",
      "- **Chemically Meaningless Patterns**: SME may miss intrinsically chemically meaningless patterns learned from data artifacts.\n",
      "- **Limited Fragment Types**: Currently, SME only supports BRICS, Murcko, and functional groups, excluding other substructures like bioisosteres.\n",
      "\n",
      "#### Conclusion\n",
      "SME offers a powerful tool for chemists to gain a deeper understanding of molecular property predictions and guide structural optimization. Its ability to provide chemically meaningful explanations makes it a valuable asset in drug discovery and molecular design. Despite some limitations, SME's intuitive and insightful interpretability aligns well with chemists' working styles and enhances the practical utility of GNN models.\n",
      "\n",
      "---\n",
      "\n",
      "This summary encapsulates the main findings, methodology, results, and implications of the study, making the core insights accessible to a general audience.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"### Summary of the Research Paper: Substructure-Mask Explanation (SME) for Molecular Property Predictions\\n\\n#### Introduction\\nThis study introduces **Substructure-Mask Explanation (SME)**, a novel perturbation-based method designed to explain predictions from Graph Neural Networks (GNNs) in terms of chemically meaningful substructures rather than individual atoms or bonds. The goal is to provide chemists with a tool that aligns with their intuition and enhances the interpretability of molecular property predictions.\\n\\n#### Background\\n- Current methods for interpreting GNNs often focus on individual nodes, edges, or features, which may not align with chemists' understanding of molecular properties.\\n- Chemists typically comprehend molecular properties through chemically meaningful fragments, such as functional groups, rather than individual atoms or bonds.\\n\\n#### Methodology\\n- **Fragmentation Methods**: SME uses three different methods for molecular fragmentation—**BRICS substructures**, **Murcko substructures**, and **functional groups**.\\n- **Attribution Calculation**: SME calculates the attribution of each substructure by masking it and observing the change in the GNN model's prediction. The attribution score is normalized between 0 and 1.\\n- **Combination Strategy**: To avoid missing important substructures, SME combines and masks substructures, calculating attributions for each combination. This helps in identifying the most crucial substructures responsible for the model's prediction.\\n\\n#### Results\\n- **Performance Metrics**: Four consensus models were developed for different molecular property predictions:\\n  - **ESOL (Aqueous Solubility)**: Achieved an \\\\( R^2 \\\\) of 0.927.\\n  - **Mutagenicity**: Achieved an ROC-AUC of 0.901.\\n  - **hERG (Cardiotoxicity)**: Achieved an ROC-AUC of 0.862.\\n  - **BBBP (Blood-Brain Barrier Permeation)**: Achieved an ROC-AUC of 0.919.\\n- **Case Studies**:\\n  - **Solubility**: SME identified hydroxyl groups as beneficial for hydrophilicity and alkyl groups as beneficial for hydrophobicity.\\n  - **Mutagenicity**: SME correctly identified toxicophores (e.g., nitro, amino, quinone groups) and detoxifying groups (e.g., carboxyl group).\\n  - **hERG Toxicity**: SME helped in identifying structural optimization strategies (e.g., reducing lipophilicity, adding hydroxyl groups).\\n  - **BBBP**: SME established correlations between substructures and physiochemical properties, such as LogP and TPSA.\\n\\n#### Implications\\n- **Enhanced Interpretability**: SME provides chemists with a more intuitive and chemically meaningful way to understand GNN predictions.\\n- **Structural Optimization**: The attribution analysis guides chemists in optimizing molecular structures for desired properties.\\n- **Model Diagnostics**: SME can help identify issues in training data and suggest improvements to enhance model reliability.\\n- **Molecular Generation**: SME can be used to generate molecules with desired properties by recombining BRICS fragments with assigned attributions.\\n\\n#### Limitations\\n- **Data Paucity**: SME may struggle to extract meaningful SAR information when the training data is limited or biased.\\n- **Chemically Meaningless Patterns**: SME may miss intrinsically chemically meaningless patterns learned from data artifacts.\\n- **Limited Fragment Types**: Currently, SME only supports BRICS, Murcko, and functional groups, excluding other substructures like bioisosteres.\\n\\n#### Conclusion\\nSME offers a powerful tool for chemists to gain a deeper understanding of molecular property predictions and guide structural optimization. Its ability to provide chemically meaningful explanations makes it a valuable asset in drug discovery and molecular design. Despite some limitations, SME's intuitive and insightful interpretability aligns well with chemists' working styles and enhances the practical utility of GNN models.\\n\\n---\\n\\nThis summary encapsulates the main findings, methodology, results, and implications of the study, making the core insights accessible to a general audience.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(prompt, input_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 领域总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 直接联网"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    " \"Act as an academic Research Expert. Conduct an extensive search for research papers on the specified [topic]. Ensure the papers are from reputable journals, conferences, or academic institutions. Provide a comprehensive list of the findings, including the title of the paper, authors, publication date, abstract, and a link to access the full paper. For each paper, write a brief summary highlighting the main findings and their relevance.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_content=\"\"\"\n",
    "GNN\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Graph Neural Networks (GNNs) have become increasingly popular in recent years due to their ability to handle graph-structured data effectively. Below is a list of some key research papers on GNNs, along with their details and summaries.\n",
      "\n",
      "### 1. **Paper Title:** \n",
      "   - Semi-Supervised Classification with Graph Convolutional Networks\n",
      "   - **Authors:** Thomas N. Kipf and Max Welling\n",
      "   - **Publication Date:** 2017\n",
      "   - **Abstract:** We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. By using a first-order approximation of localized spectral filtering on graphs, we develop a model that operates efficiently on large graphs.\n",
      "   - **Link:** https://arxiv.org/abs/1609.02907\n",
      "\n",
      "   **Summary:** This paper introduces Graph Convolutional Networks (GCNs), which are designed to efficiently process large-scale graph data. The authors propose a simplified version of spectral graph convolutions that can be computed in linear time, making it suitable for large graphs. The model achieves state-of-the-art performance on several benchmark datasets for node classification tasks.\n",
      "\n",
      "### 2. **Paper Title:**\n",
      "   - Inductive Representation Learning on Large Graphs\n",
      "   - **Authors:** William L. Hamilton, Rex Ying, and Jure Leskovec\n",
      "   - **Publication Date:** 2017\n",
      "   - **Abstract:** Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require knowing the entire graph, making them infeasible for large graphs. Here we propose GraphSAGE, a general, inductive framework that leverages node feature information to efficiently generate node embeddings for previously unseen data.\n",
      "   - **Link:** https://arxiv.org/abs/1706.02216\n",
      "\n",
      "   **Summary:** This paper presents GraphSAGE, a method that generates node embeddings by sampling and aggregating information from the neighborhood of each node. Unlike previous methods, GraphSAGE is inductive, meaning it can generate embeddings for new nodes without retraining the model. This makes it particularly useful for dynamic or large-scale graphs.\n",
      "\n",
      "### 3. **Paper Title:**\n",
      "   - Hierarchical Graph Pooling with Structure Learning\n",
      "   - **Authors:** Zhiqiang Zhong, Yu Rong, Tingyang Xu, Wenbing Huang, and Junzhou Huang\n",
      "   - **Publication Date:** 2019\n",
      "   - **Abstract:** Graph pooling is an essential component in graph representation learning. Most existing methods rely on a pre-defined pooling strategy, which often ignores the hierarchical structure and the local connectivity of the graph. To address this issue, we propose a novel hierarchical graph pooling method called HGP, which learns the pooling strategy and the structure simultaneously.\n",
      "   - **Link:** https://arxiv.org/abs/1911.08532\n",
      "\n",
      "   **Summary:** This paper introduces Hierarchical Graph Pooling (HGP), a method that learns both the pooling strategy and the structure of the graph simultaneously. By incorporating structure learning into the pooling process, HGP can better capture the hierarchical and local connectivity patterns within graphs, leading to improved performance in graph classification tasks.\n",
      "\n",
      "### 4. **Paper Title:**\n",
      "   - Graph Attention Networks\n",
      "   - **Authors:** Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio\n",
      "   - **Publication Date:** 2018\n",
      "   - **Abstract:** We present Graph Attention Networks (GATs), a family of attention-based neural network models for learning tasks involving graph-structured data. GATs leverage masked self-attention layers, allowing nodes to attend over their neighborhoods' features, implicitly weighting neighbor structural context.\n",
      "   - **Link:** https://arxiv.org/abs/1710.10903\n",
      "\n",
      "   **Summary:** This paper introduces Graph Attention Networks (GATs), which use attention mechanisms to weigh the importance of different neighbors when aggregating information. This allows GATs to focus on more relevant parts of the graph, improving their ability to capture complex dependencies and relationships within the graph.\n",
      "\n",
      "### 5. **Paper Title:**\n",
      "   - Fast Graph Representation Learning with PyTorch Geometric\n",
      "   - **Authors:** Matthias Fey and Jan E. Lenssen\n",
      "   - **Publication Date:** 2019\n",
      "   - **Abstract:** We present PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds, and manifolds, built upon PyTorch. In addition to providing the necessary data structures to work with such data, it implements a wide range of recently published methods.\n",
      "   - **Link:** https://arxiv.org/abs/1903.05290\n",
      "\n",
      "   **Summary:** This paper describes PyTorch Geometric, a library designed to facilitate the implementation and training of deep learning models on graph-structured data. It provides a suite of tools and modules that simplify the process of working with graph data, making it easier for researchers and practitioners to experiment with and deploy GNNs.\n",
      "\n",
      "These papers represent some of the key advancements in the field of Graph Neural Networks, covering topics such as scalable training, inductive learning, hierarchical pooling, attention mechanisms, and software libraries. Each paper contributes to the growing body of knowledge on how to effectively process and analyze graph-structured data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Certainly! Graph Neural Networks (GNNs) have become increasingly popular in recent years due to their ability to handle graph-structured data effectively. Below is a list of some key research papers on GNNs, along with their details and summaries.\\n\\n### 1. **Paper Title:** \\n   - Semi-Supervised Classification with Graph Convolutional Networks\\n   - **Authors:** Thomas N. Kipf and Max Welling\\n   - **Publication Date:** 2017\\n   - **Abstract:** We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. By using a first-order approximation of localized spectral filtering on graphs, we develop a model that operates efficiently on large graphs.\\n   - **Link:** https://arxiv.org/abs/1609.02907\\n\\n   **Summary:** This paper introduces Graph Convolutional Networks (GCNs), which are designed to efficiently process large-scale graph data. The authors propose a simplified version of spectral graph convolutions that can be computed in linear time, making it suitable for large graphs. The model achieves state-of-the-art performance on several benchmark datasets for node classification tasks.\\n\\n### 2. **Paper Title:**\\n   - Inductive Representation Learning on Large Graphs\\n   - **Authors:** William L. Hamilton, Rex Ying, and Jure Leskovec\\n   - **Publication Date:** 2017\\n   - **Abstract:** Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require knowing the entire graph, making them infeasible for large graphs. Here we propose GraphSAGE, a general, inductive framework that leverages node feature information to efficiently generate node embeddings for previously unseen data.\\n   - **Link:** https://arxiv.org/abs/1706.02216\\n\\n   **Summary:** This paper presents GraphSAGE, a method that generates node embeddings by sampling and aggregating information from the neighborhood of each node. Unlike previous methods, GraphSAGE is inductive, meaning it can generate embeddings for new nodes without retraining the model. This makes it particularly useful for dynamic or large-scale graphs.\\n\\n### 3. **Paper Title:**\\n   - Hierarchical Graph Pooling with Structure Learning\\n   - **Authors:** Zhiqiang Zhong, Yu Rong, Tingyang Xu, Wenbing Huang, and Junzhou Huang\\n   - **Publication Date:** 2019\\n   - **Abstract:** Graph pooling is an essential component in graph representation learning. Most existing methods rely on a pre-defined pooling strategy, which often ignores the hierarchical structure and the local connectivity of the graph. To address this issue, we propose a novel hierarchical graph pooling method called HGP, which learns the pooling strategy and the structure simultaneously.\\n   - **Link:** https://arxiv.org/abs/1911.08532\\n\\n   **Summary:** This paper introduces Hierarchical Graph Pooling (HGP), a method that learns both the pooling strategy and the structure of the graph simultaneously. By incorporating structure learning into the pooling process, HGP can better capture the hierarchical and local connectivity patterns within graphs, leading to improved performance in graph classification tasks.\\n\\n### 4. **Paper Title:**\\n   - Graph Attention Networks\\n   - **Authors:** Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio\\n   - **Publication Date:** 2018\\n   - **Abstract:** We present Graph Attention Networks (GATs), a family of attention-based neural network models for learning tasks involving graph-structured data. GATs leverage masked self-attention layers, allowing nodes to attend over their neighborhoods' features, implicitly weighting neighbor structural context.\\n   - **Link:** https://arxiv.org/abs/1710.10903\\n\\n   **Summary:** This paper introduces Graph Attention Networks (GATs), which use attention mechanisms to weigh the importance of different neighbors when aggregating information. This allows GATs to focus on more relevant parts of the graph, improving their ability to capture complex dependencies and relationships within the graph.\\n\\n### 5. **Paper Title:**\\n   - Fast Graph Representation Learning with PyTorch Geometric\\n   - **Authors:** Matthias Fey and Jan E. Lenssen\\n   - **Publication Date:** 2019\\n   - **Abstract:** We present PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds, and manifolds, built upon PyTorch. In addition to providing the necessary data structures to work with such data, it implements a wide range of recently published methods.\\n   - **Link:** https://arxiv.org/abs/1903.05290\\n\\n   **Summary:** This paper describes PyTorch Geometric, a library designed to facilitate the implementation and training of deep learning models on graph-structured data. It provides a suite of tools and modules that simplify the process of working with graph data, making it easier for researchers and practitioners to experiment with and deploy GNNs.\\n\\nThese papers represent some of the key advancements in the field of Graph Neural Networks, covering topics such as scalable training, inductive learning, hierarchical pooling, attention mechanisms, and software libraries. Each paper contributes to the growing body of knowledge on how to effectively process and analyze graph-structured data.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(prompt,input_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 多篇文章总结作为输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "\"Act as an academic research expert. Read and digest the content of the research paper titled [title]. Produce a concise and clear summary that encapsulates the main findings, methodology, results, and implications of the study. Ensure that the summary is written in a manner that is accessible to a general audience while retaining the core insights and nuances of the original paper.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File dudev-lim-2013-competition-among-metal-ions-for-protein-binding-sites-determinants-of-metal-ion-selectivity-in-proteins_main_text.md already processed. Skipping...\n",
      "File 2023-Chemistry-intuitive explanation of graph neural networks for molecular property prediction with substructure masking_main_text.md already processed. Skipping...\n",
      "Summaries saved to summaries.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# 定义 work 文件夹路径\n",
    "work_folder = './work'\n",
    "\n",
    "# 获取 work 文件夹下所有 .md 文件\n",
    "file_paths = glob.glob(os.path.join(work_folder, '*.md'))\n",
    "\n",
    "# 定义 JSON 文件路径\n",
    "output_file = 'summaries.json'\n",
    "\n",
    "# 加载已有的 JSON 数据（如果文件存在）\n",
    "if os.path.exists(output_file):\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as json_file:\n",
    "            results = json.load(json_file)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON file: {output_file}. Starting with an empty list.\")\n",
    "        results = []\n",
    "else:\n",
    "    results = []\n",
    "\n",
    "# 提取已处理文件的名称列表\n",
    "processed_files = {entry['file_name'] for entry in results}\n",
    "\n",
    "# 遍历文件路径并读取内容\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)  # 提取文件名\n",
    "    if file_name in processed_files:\n",
    "        print(f\"File {file_name} already processed. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # 检查文件是否存在\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                # 读取文件内容\n",
    "                file_content = file.read()\n",
    "                # 调用 generate_response 生成回复\n",
    "                prompt = \"Summarize the content:\"\n",
    "                full_response = generate_response(prompt, file_content)\n",
    "                # 将结果存入字典\n",
    "                results.append({\n",
    "                    \"file_name\": file_name,\n",
    "                    \"gpt_summary\": full_response\n",
    "                })\n",
    "                # 打印生成的内容\n",
    "                print(f\"Processed file: {file_path}\")\n",
    "        else:\n",
    "            print(f\"File not found at the path: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# 将结果存储到 JSON 文件\n",
    "try:\n",
    "    with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(results, json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"Summaries saved to {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 2023-Chemistry-intuitive explanation of graph neural networks for molecular property prediction with substructure masking_main_text.md\n",
      "Summary: This text discusses the use of Graph Neural Networks (GNNs) for predicting molecular properties and introduces a new method called Substructure-Mask Explanation (SME) to improve the interpretability of these predictions. Key points include:\n",
      "\n",
      "1. **Molecular Representations**: Molecular graphs are commonly used in GNNs to represent chemical structures, but the choice of representation can limit model performance and explainability.\n",
      "\n",
      "2. **Current Interpretation Methods**: Various methods exist to interpret GNN predictions, including gradients-based, decomposition, surrogate, generation-based, and perturbation-based methods. However, most focus on individual nodes, edges, or features rather than chemically meaningful substructures like functional groups.\n",
      "\n",
      "3. **Limitations of Current Methods**: These methods often fail to align with chemists' intuition, as they do not consider substructures that are meaningful in chemical contexts. For instance, perturbation-based methods may split functional groups into individual atoms, which does not reflect actual chemical behavior.\n",
      "\n",
      "4. **Introduction of SME**: SME is a perturbation-based method that identifies the most crucial substructures in a molecule responsible for a model's prediction. It uses different molecular fragmentation methods (BRICS substructures, Murcko substructures, and functional groups) to provide chemically intuitive explanations.\n",
      "\n",
      "5. **Applications of SME**: SME is applied to four molecular property prediction tasks: aqueous solubility, mutagenicity, hERG-related cardiotoxicity, and blood-brain barrier permeation. It helps in:\n",
      "   - Understanding how specific substructures contribute to property predictions.\n",
      "   - Guiding structural optimization for medicinal chemists.\n",
      "   - Identifying problematic areas in the model's learning process, such as insufficient or imbalanced training data.\n",
      "   - Generating molecules with desired properties by recombining fragments with specific attributions.\n",
      "\n",
      "6. **Effectiveness of SME**: SME demonstrates its effectiveness by showing how different substructures (e.g., hydroxyl groups for solubility, aromatic nitro groups for mutagenicity) influence model predictions. It also helps in structural optimization and generates molecules with desired properties.\n",
      "\n",
      "7. **Challenges and Limitations**: While SME provides chemically meaningful explanations, it faces challenges such as handling data paucity or bias and ensuring that intrinsically chemically meaningless patterns are not overlooked. Additionally, SME currently only supports specific substructures and does not assess others like bioisosteres.\n",
      "\n",
      "Overall, SME offers a more chemically intuitive way to interpret GNN predictions, making it a valuable tool for chemists in understanding and optimizing molecular properties.\n",
      "\n",
      "File: dudev-lim-2013-competition-among-metal-ions-for-protein-binding-sites-determinants-of-metal-ion-selectivity-in-proteins_main_text.md\n",
      "Summary: The text discusses the critical role of metal cations in cellular biochemistry and biophysics, focusing on how these cations are selected and protected within proteins. Key points include:\n",
      "\n",
      "1. **Importance of Metal Cations**: Metal cations are essential for various cellular functions, including enzyme catalysis, signal transduction, and structural stabilization.\n",
      "   \n",
      "2. **Selection Mechanisms**:\n",
      "   - **Metal Properties**: Factors like valence state, ionic radius, and charge-accepting ability influence metal binding.\n",
      "   - **Ligand Characteristics**: Net charge, dipole moment, and charge-donating ability of ligands affect metal binding.\n",
      "   - **Protein Environment**: The protein matrix and second-shell ligands can stabilize metal-binding sites and enhance metal affinity.\n",
      "\n",
      "3. **Metal Competition**:\n",
      "   - **Monovalent vs Divalent Ions**: Sodium (Na+) and potassium (K+) channels select K+ over Na+ due to differences in hydration penalties, coordination numbers, and pore rigidity.\n",
      "   - **Transition Metals**: Specific factors like coordination geometry and binding site architecture determine selectivity among transition metals like Zn2+, Cu2+, and others.\n",
      "\n",
      "4. **Homeostasis and Regulation**:\n",
      "   - Cells maintain metal homeostasis through mechanisms like metal uptake proteins, transport proteins, and storage proteins.\n",
      "   - High-affinity binding and low cellular concentrations of certain metals (like Cu2+ and Zn2+) prevent their displacement by stronger competitors like Mg2+.\n",
      "\n",
      "5. **Examples of Metal Selectivity**:\n",
      "   - **Ion Channels**: Potassium channels select K+ over Na+.\n",
      "   - **Calcium Proteins**: EF-hand motifs prefer Ca2+ over Mg2+.\n",
      "   - **Zinc Enzymes**: Zinc-dependent enzymes like carbonic anhydrase II select Zn2+ over other transition metals.\n",
      "\n",
      "6. **Challenges and Future Directions**:\n",
      "   - Understanding the mechanisms of metal selectivity and the roles of metalloregulatory proteins.\n",
      "   - Investigating the impact of metal ions on disease and developing strategies for treating metal-related disorders.\n",
      "\n",
      "Overall, the text emphasizes the intricate interplay between metal properties, protein environments, and cellular mechanisms in ensuring the correct metal cation is selected and maintained for optimal protein function.\n",
      "\n",
      "\n",
      "### Research Summary: Molecular Property Prediction and Metal Ion Binding\n",
      "\n",
      "#### Molecular Property Prediction Using Graph Neural Networks (GNNs)\n",
      "1. **Molecular Representation**: GNNs use molecular graphs to represent chemical structures, but the choice of representation impacts model performance and explainability.\n",
      "2. **Interpretation Methods**: Existing methods (gradients-based, decomposition, surrogate, generation-based, and perturbation-based) often fail to align with chemists' intuition because they focus on individual nodes, edges, or features rather than chemically meaningful substructures like functional groups.\n",
      "3. **Substructure-Mask Explanation (SME)**: A new perturbation-based method introduced to identify crucial substructures in a molecule responsible for model predictions. It uses different molecular fragmentation methods (BRICS substructures, Murcko substructures, and functional groups)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 定义 summaries.json 的路径\n",
    "json_file_path = 'summaries.json'\n",
    "\n",
    "# 初始化变量 sum_papers\n",
    "sum_papers = \"\"\n",
    "\n",
    "# 加载 JSON 文件\n",
    "try:\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "        summaries = json.load(json_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    summaries = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error decoding JSON file: {json_file_path}\")\n",
    "    summaries = []\n",
    "\n",
    "# 转换内容为文本格式\n",
    "if summaries:\n",
    "    for entry in summaries:\n",
    "        file_name = entry.get(\"file_name\", \"Unknown file\")\n",
    "        gpt_summary = entry.get(\"gpt_summary\", \"\")\n",
    "        # 添加到 sum_papers\n",
    "        sum_papers += f\"File: {file_name}\\nSummary: {gpt_summary}\\n\\n\"\n",
    "else:\n",
    "    sum_papers = \"No summaries available.\"\n",
    "\n",
    "# 打印 sum_papers 内容（可选）\n",
    "#print(sum_papers)\n",
    "prompt=\"\"\"请根据我提供的多篇文献总结，归纳一下这个领域的研究现状\"\"\"\n",
    "generate_response(prompt, sum_papers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
